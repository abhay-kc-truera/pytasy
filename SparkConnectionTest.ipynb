{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5b42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/13 08:18:42 WARN Utils: Your hostname, Chaturvedi_PC resolves to a loopback address: 127.0.1.1; using 172.27.71.32 instead (on interface eth0)\n",
      "23/02/13 08:18:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/02/13 08:18:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com')\\\n",
    "    .config('spark.driver.bindAddress','localhost')\\\n",
    "    .config(\"spark.ui.port\",\"4051\")\\\n",
    "    .config(\"spark.driver.memory\",\"5g\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac474655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = '/home/abhay/work/dream11/processed_output/training_rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e61f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/13 08:18:53 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(train_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4551281",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_df = train_df.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8585bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def __extract_batting_fantasy_points(\n",
    "        batter_runs: Optional[int], dismissals: Optional[int],\n",
    "        balls_faced: Optional[int], boundaries_count: Optional[int], sixes_count: Optional[int] ):\n",
    "    fantasy_points = 0\n",
    "    is_out: bool = dismissals is not None and dismissals > 0\n",
    "    is_duck = batter_runs is not None and batter_runs==0 and is_out\n",
    "    if batter_runs is None:\n",
    "        batter_runs = 0\n",
    "    if balls_faced is None:\n",
    "        balls_faced = 0\n",
    "    \n",
    "    strike_rate = batter_runs*100.0/balls_faced if balls_faced > 0 else 100\n",
    "    fantasy_points = batter_runs * 1 \n",
    "    if boundaries_count is not None:\n",
    "        fantasy_points += boundaries_count * 1\n",
    "    if sixes_count is not None:\n",
    "        fantasy_points += sixes_count * 2\n",
    "    # handle half century\n",
    "    if batter_runs >=50 and batter_runs < 100:\n",
    "        fantasy_points += 8\n",
    "    # handle century\n",
    "    elif batter_runs>=100:\n",
    "        fantasy_points += 16\n",
    "    # strike rate penalty\n",
    "    if strike_rate >=60 and strike_rate<=70:\n",
    "        fantasy_points -= 2\n",
    "    elif strike_rate >=50 and strike_rate<60:\n",
    "        fantasy_points -= 4\n",
    "    elif strike_rate <50:\n",
    "        fantasy_points -= 6\n",
    "    # duck penalty\n",
    "    if is_duck:\n",
    "        fantasy_points -= 2\n",
    "    return fantasy_points\n",
    "\n",
    "def __extract_bowling_fantasy_points(total_runs: Optional[int], wickets: Optional[int], deliveries: Optional[int], maidens: Optional[int]):\n",
    "    fantasy_points = 0\n",
    "    if total_runs is None:\n",
    "        total_runs = 0\n",
    "    if deliveries is None:\n",
    "        deliveries = 0\n",
    "    if wickets is None:\n",
    "        wickets = 0\n",
    "    if maidens is None:\n",
    "        maidens = 0\n",
    "    economy_rate = total_runs*6.0/deliveries if deliveries > 0 else 6\n",
    "    fantasy_points = wickets * 25 + maidens * 8\n",
    "    # handle 4 wickets\n",
    "    if wickets >=4 and wickets < 5:\n",
    "        fantasy_points += 8\n",
    "    # handle 5 wickets\n",
    "    elif wickets>5:\n",
    "        fantasy_points += 16\n",
    "    # handle economy bonus\n",
    "    if economy_rate >= 5 and economy_rate < 6:\n",
    "        fantasy_points += 2\n",
    "    elif economy_rate < 5:\n",
    "        fantasy_points += 4\n",
    "    return fantasy_points\n",
    "\n",
    "def __extract_fielding_fantasy_points(fielding_wickets: Optional[int]):\n",
    "    fantasy_points = 0\n",
    "    fantasy_points += fielding_wickets * 7 if fielding_wickets is not None else 0 # avergae to 7 to account for various dismisaals mechanisms\n",
    "    return fantasy_points\n",
    "\n",
    "def get_fantasy_points(\n",
    "        batter_runs: Optional[int], dismissals: Optional[int], balls_faced: Optional[int], boundaries_count: Optional[int], \n",
    "        sixes_count: Optional[int],\n",
    "        total_runs: Optional[int], wickets: Optional[int], deliveries: Optional[int], maidens: Optional[int],\n",
    "        fielding_wickets: Optional[int]\n",
    "    ):\n",
    "    return __extract_batting_fantasy_points(batter_runs, dismissals, balls_faced, boundaries_count, sixes_count) \\\n",
    "        + __extract_bowling_fantasy_points(total_runs, wickets, deliveries, maidens) \\\n",
    "        + __extract_fielding_fantasy_points(fielding_wickets) \\\n",
    "        + 4 # 4 points for being selected\n",
    "    \n",
    "get_fantasy_points_udf = udf(get_fantasy_points, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c83883",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = limit_df.withColumn(\"some_Col\", get_fantasy_points_udf(\n",
    "            limit_df.batter_run_sum, limit_df.dismissals, limit_df.balls_faced,\n",
    "            limit_df.boundary_count, limit_df.six_count,\n",
    "            limit_df.total_run_sum, limit_df.wicket_sum, limit_df.deliveries, limit_df.maiden_count,\n",
    "            limit_df.fielding_wicket_sum\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3fd1607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batter_run_sum</th>\n",
       "      <th>dismissals</th>\n",
       "      <th>balls_faced</th>\n",
       "      <th>boundary_count</th>\n",
       "      <th>six_count</th>\n",
       "      <th>total_run_sum</th>\n",
       "      <th>wicket_sum</th>\n",
       "      <th>deliveries</th>\n",
       "      <th>maiden_count</th>\n",
       "      <th>fielding_wicket_sum</th>\n",
       "      <th>some_Col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    batter_run_sum  dismissals  balls_faced  boundary_count  six_count  \\\n",
       "0            101.0         1.0         55.0             5.0        8.0   \n",
       "1              4.0         1.0          7.0             1.0        0.0   \n",
       "2              NaN         NaN          NaN             NaN        NaN   \n",
       "3             10.0         1.0         18.0             0.0        0.0   \n",
       "4             22.0         0.0         14.0             4.0        0.0   \n",
       "..             ...         ...          ...             ...        ...   \n",
       "95            13.0         1.0         13.0             2.0        0.0   \n",
       "96             0.0         1.0          1.0             0.0        0.0   \n",
       "97             3.0         0.0          3.0             0.0        0.0   \n",
       "98             6.0         1.0          7.0             0.0        0.0   \n",
       "99             0.0         0.0          5.0             0.0        0.0   \n",
       "\n",
       "    total_run_sum  wicket_sum  deliveries  maiden_count  fielding_wicket_sum  \\\n",
       "0            28.0         3.0        25.0           0.0                  1.0   \n",
       "1             NaN         NaN         NaN           NaN                  NaN   \n",
       "2            26.0         1.0        24.0           0.0                  1.0   \n",
       "3            25.0         3.0        24.0           0.0                  NaN   \n",
       "4            16.0         0.0        16.0           0.0                  NaN   \n",
       "..            ...         ...         ...           ...                  ...   \n",
       "95            NaN         NaN         NaN           NaN                  1.0   \n",
       "96           24.0         1.0        24.0           0.0                  1.0   \n",
       "97           11.0         0.0        12.0           0.0                  NaN   \n",
       "98            NaN         NaN         NaN           NaN                  NaN   \n",
       "99           62.0         2.0        26.0           0.0                  NaN   \n",
       "\n",
       "    some_Col  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "..       ...  \n",
       "95       NaN  \n",
       "96       NaN  \n",
       "97       NaN  \n",
       "98       NaN  \n",
       "99       NaN  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['batter_run_sum', 'dismissals', 'balls_faced',\n",
    "            'boundary_count', 'six_count',\n",
    "            'total_run_sum', 'wicket_sum', 'deliveries', 'maiden_count',\n",
    "            'fielding_wicket_sum', \"some_Col\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7942d3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+--------------+---------+-------------+----------+----------+------------+-------------------+--------------+--------+\n",
      "|batter_run_sum|dismissals|balls_faced|boundary_count|six_count|total_run_sum|wicket_sum|deliveries|maiden_count|fielding_wicket_sum|fantasy_points|some_Col|\n",
      "+--------------+----------+-----------+--------------+---------+-------------+----------+----------+------------+-------------------+--------------+--------+\n",
      "|           101|         1|         55|             5|        8|           28|         3|        25|           0|                  1|          null|     224|\n",
      "|             4|         1|          7|             1|        0|         null|      null|      null|        null|               null|          null|       5|\n",
      "|          null|      null|       null|          null|     null|           26|         1|        24|           0|                  1|          null|      36|\n",
      "|            10|         1|         18|             0|        0|           25|         3|        24|           0|               null|          null|      85|\n",
      "|            22|         0|         14|             4|        0|           16|         0|        16|           0|               null|          null|      30|\n",
      "|             9|         1|          6|             0|        1|           16|         0|        16|           0|                  1|          null|      22|\n",
      "|             6|         0|          6|             1|        0|           13|         1|        10|           0|               null|          null|      36|\n",
      "|             4|         0|          1|             1|        0|           20|         2|        24|           0|               null|          null|      61|\n",
      "|            10|         1|         15|             0|        0|           17|         1|        18|           0|                  1|          null|      46|\n",
      "|          null|      null|       null|          null|     null|           30|         2|        24|           0|               null|          null|      54|\n",
      "|             1|         0|          1|             0|        0|           25|         3|        19|           0|                  1|          null|      87|\n",
      "|            32|         1|         31|             6|        0|           29|         0|        18|           0|               null|          null|      42|\n",
      "|            15|         1|         10|             1|        1|           33|         0|        19|           0|               null|          null|      22|\n",
      "|            19|         1|         16|             0|        2|         null|      null|      null|        null|                  1|          null|      34|\n",
      "|            16|         0|          5|             0|        2|           22|         1|        13|           0|               null|          null|      49|\n",
      "|             1|         1|          4|             0|        0|           27|         0|        18|           0|               null|          null|      -1|\n",
      "|             3|         1|          7|             0|        0|            3|         2|         4|           0|                  3|          null|      76|\n",
      "|             0|         1|          1|             0|        0|           27|         2|        18|           0|               null|          null|      46|\n",
      "|            18|         1|         22|             3|        0|           34|         0|        26|           0|                  1|          null|      32|\n",
      "|             0|         1|          1|             0|        0|           27|         2|        18|           0|               null|          null|      46|\n",
      "+--------------+----------+-----------+--------------+---------+-------------+----------+----------+------------+-------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/13 08:44:17 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:182)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:178)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:178)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:173)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1931)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/02/13 08:44:17 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e/27. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e/27\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:182)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:178)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:178)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:173)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1931)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/02/13 08:44:17 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e/0a. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-8117d7b3-f9f2-4991-8937-28a164c0580e/0a\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:182)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:178)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:178)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:173)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1931)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58038)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/abhay/work/miniconda3/envs/dream11/lib/python3.9/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_df.select(['batter_run_sum', 'dismissals', 'balls_faced',\n",
    "            'boundary_count', 'six_count',\n",
    "            'total_run_sum', 'wicket_sum', 'deliveries', 'maiden_count',\n",
    "            'fielding_wicket_sum', \"fantasy_points\",\"some_Col\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afe4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
