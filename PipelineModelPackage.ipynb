{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6949125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.spark.spark_session_helper import spark\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor ,GBTRegressionModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "features = [\n",
    "     'is_male',\n",
    "     'batter_runs_30D',\n",
    "     'batter_runs_90D',\n",
    "     'batter_runs_300D',\n",
    "     'batter_runs_1000D',\n",
    "     'batter_runs_1000D_venue',\n",
    "     'balls_faced_30D',\n",
    "     'balls_faced_90D',\n",
    "     'balls_faced_300D',\n",
    "     'balls_faced_1000D',\n",
    "     'balls_faced_1000D_venue',\n",
    "     'dismissals_30D',\n",
    "     'dismissals_90D',\n",
    "     'dismissals_300D',\n",
    "     'dismissals_1000D',\n",
    "     'dismissals_1000D_venue',\n",
    "     'boundary_count_30D',\n",
    "     'boundary_count_90D',\n",
    "     'boundary_count_300D',\n",
    "     'boundary_count_1000D',\n",
    "     'boundary_count_1000D_venue',\n",
    "     'six_count_30D',\n",
    "     'six_count_90D',\n",
    "     'six_count_300D',\n",
    "     'six_count_1000D',\n",
    "     'six_count_1000D_venue',\n",
    "     'batting_avg_30D',\n",
    "     'batting_avg_90D',\n",
    "     'batting_avg_300D',\n",
    "     'batting_avg_1000D',\n",
    "     'batting_avg_1000D_venue',\n",
    "     'batting_sr_30D',\n",
    "     'batting_sr_90D',\n",
    "     'batting_sr_300D',\n",
    "     'batting_sr_1000D',\n",
    "     'batting_sr_1000D_venue',\n",
    "     'total_runs_30D',\n",
    "     'total_runs_90D',\n",
    "     'total_runs_300D',\n",
    "     'total_runs_1000D',\n",
    "     'total_runs_1000D_venue',\n",
    "     'deliveries_30D',\n",
    "     'deliveries_90D',\n",
    "     'deliveries_300D',\n",
    "     'deliveries_1000D',\n",
    "     'deliveries_1000D_venue',\n",
    "     'wicket_sum_30D',\n",
    "     'wicket_sum_90D',\n",
    "     'wicket_sum_300D',\n",
    "     'wicket_sum_1000D',\n",
    "     'wicket_sum_1000D_venue',\n",
    "     'maiden_count_30D',\n",
    "     'maiden_count_90D',\n",
    "     'maiden_count_300D',\n",
    "     'maiden_count_1000D',\n",
    "     'maiden_count_1000D_venue',\n",
    "     'bowling_avg_30D',\n",
    "     'bowling_avg_90D',\n",
    "     'bowling_avg_300D',\n",
    "     'bowling_avg_1000D',\n",
    "     'bowling_avg_1000D_venue',\n",
    "     'bowling_sr_30D',\n",
    "     'bowling_sr_90D',\n",
    "     'bowling_sr_300D',\n",
    "     'bowling_sr_1000D',\n",
    "     'bowling_sr_1000D_venue',\n",
    "     'bowling_eco_30D',\n",
    "     'bowling_eco_90D',\n",
    "     'bowling_eco_300D',\n",
    "     'bowling_eco_1000D',\n",
    "     'bowling_eco_1000D_venue',\n",
    "     'fielding_wicket_sum'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_manager import model_train_input_path, model_test_input_path, model_train_predictions_path, model_test_predictions_path, model_pipeline_save_artifact_path\n",
    "\n",
    "train_df = spark.read.parquet(model_train_input_path)\n",
    "test_df = spark.read.parquet(model_test_input_path)\n",
    "print(\"Starting Model Training Flow\")\n",
    "# assemble features\n",
    "vector_assembler = VectorAssembler(inputCols = features, outputCol='features')\n",
    "# GBT regressor\n",
    "gbt_regressor = GBTRegressor(featuresCol='features', labelCol='fantasy_points', maxIter=10)\n",
    "# create Pipeline\n",
    "pipeline = Pipeline(stages = [vector_assembler, gbt_regressor])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "print(\"Pipeline model trained\")\n",
    "# save model\n",
    "pipeline_model.write().overwrite().save(model_pipeline_save_artifact_path)\n",
    "\n",
    "print(\"model saved\")\n",
    "\n",
    "# load saved model\n",
    "loaded_pipeline_model = PipelineModel.load(model_pipeline_save_artifact_path)\n",
    "\n",
    "print(\"loaded saved model\")\n",
    "\n",
    "# assemble test features and predict on test data\n",
    "test_predictions = loaded_pipeline_model.transform(test_df)\n",
    "test_predictions.limit(100).toPandas()\n",
    "\n",
    "# # write predictions to disk\n",
    "# print(\"writing predictions over test input data to disk\")\n",
    "# test_predictions.write.format(\"parquet\").partitionBy([\"dt\"]).mode(\"overwrite\").save(model_test_predictions_path)\n",
    "\n",
    "# print(\"writing predictions over train input data to disk\")\n",
    "# train_predictions.write.format(\"parquet\").partitionBy([\"dt\"]).mode(\"overwrite\").save(model_train_predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_manager import model_pipeline_save_artifact_path, model_pipeline_pmml_save_artifact_path\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "\n",
    "pmmlBuilder = PMMLBuilder(spark, train_df, loaded_pipeline_model)\n",
    "\n",
    "pmmlBuilder.buildFile(model_pipeline_pmml_save_artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e448626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = spark.read.parquet(\"/home/abhay/IdeaProjects/litti-ml/src/main/resources/feature-store-data/local-parquet/data.parquet\").limit(100).toPandas()\n",
    "pandas_df = pandas_df[pandas_df[\"venue_name\"].notnull()]\n",
    "pandas_df['dt'] = pd.to_datetime(pandas_df['dt'])\n",
    "pandas_df['dt'] = pandas_df['dt'].dt.strftime(\"%Y-%m-%d\")\n",
    "pandas_df = pandas_df[[\"player_id\",\"dt\",\"venue_name\"]][:20]\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_payloads = []\n",
    "for row in pandas_df[[\"player_id\",\"dt\",\"venue_name\"]][:10].to_dict(orient=\"row\"):\n",
    "    model_payloads.append({\"id\":\"request_%d\"%(len(model_payloads)),\"inputs\":row})\n",
    "print(json.dumps(model_payloads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e706271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "198575fd6629fa91bfd9fa074cc83b4a356efd4da6eb166ac1d1445fe6d3a8d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
