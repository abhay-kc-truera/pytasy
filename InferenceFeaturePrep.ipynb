{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.spark.spark_session_helper import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12593e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_manager import intermediate_data_all_train_rows_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fef609",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_player_rows_df = spark.read.parquet(intermediate_data_all_train_rows_path)\n",
    "all_player_rows_df.registerTempTable(\"all_player_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select distinct(venue_name) from all_player_rows where \n",
    "    dt>date_sub(current_timestamp(), 1000)\n",
    "    and lower(venue_name) like '%george%'\n",
    "\"\"\").limit(30).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "feature_query_template = Template(\"\"\"\n",
    "select \n",
    "    player_id, '$venue_name' as venue_name,\n",
    "    \n",
    "    is_male,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then batter_run_sum else 0 end) as batter_runs_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then batter_run_sum else 0 end) as batter_runs_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then balls_faced else 0 end) as balls_faced_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then balls_faced else 0 end) as balls_faced_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then dismissals else 0 end) as dismissals_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then dismissals else 0 end) as dismissals_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then boundary_count else 0 end) as boundary_count_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then boundary_count else 0 end) as boundary_count_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then six_count else 0 end) as six_count_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then six_count else 0 end) as six_count_1000D_venue,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then batter_run_sum else 0 end) as batter_runs_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) and venue_name='$venue_name' then batter_run_sum else 0 end) as batter_runs_300D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then balls_faced else 0 end) as balls_faced_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then dismissals else 0 end) as dismissals_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then boundary_count else 0 end) as boundary_count_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then six_count else 0 end) as six_count_300D,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then batter_run_sum else 0 end) as batter_runs_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) and venue_name='$venue_name' then batter_run_sum else 0 end) as batter_runs_90D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then balls_faced else 0 end) as balls_faced_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then dismissals else 0 end) as dismissals_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then boundary_count else 0 end) as boundary_count_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then six_count else 0 end) as six_count_90D,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then batter_run_sum else 0 end) as batter_runs_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) and venue_name='$venue_name' then batter_run_sum else 0 end) as batter_runs_30D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then balls_faced else 0 end) as balls_faced_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then dismissals else 0 end) as dismissals_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then boundary_count else 0 end) as boundary_count_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then six_count else 0 end) as six_count_30D,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then total_run_sum else 0 end) as total_runs_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then total_run_sum else 0 end) as total_runs_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then deliveries else 0 end) as deliveries_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then deliveries else 0 end) as deliveries_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then wicket_sum else 0 end) as wicket_sum_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then wicket_sum else 0 end) as wicket_sum_1000D_venue,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then maiden_count else 0 end) as maiden_count_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) and venue_name='$venue_name' then maiden_count else 0 end) as maiden_count_1000D_venue,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then total_run_sum else 0 end) as total_runs_300D,    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then deliveries else 0 end) as deliveries_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then wicket_sum else 0 end) as wicket_sum_300D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 300) then maiden_count else 0 end) as maiden_count_300D,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then total_run_sum else 0 end) as total_runs_90D,    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then deliveries else 0 end) as deliveries_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then wicket_sum else 0 end) as wicket_sum_90D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 90) then maiden_count else 0 end) as maiden_count_90D,\n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then total_run_sum else 0 end) as total_runs_30D,    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then deliveries else 0 end) as deliveries_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then wicket_sum else 0 end) as wicket_sum_30D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then maiden_count else 0 end) as maiden_count_30D,\n",
    "    \n",
    "    \n",
    "    sum(case when dt>date_sub(current_timestamp(), 1000) then fielding_wicket_sum else 0 end) as fielding_wicket_sum_1000D,\n",
    "    sum(case when dt>date_sub(current_timestamp(), 30) then fielding_wicket_sum else 0 end) as fielding_wicket_sum_30D\n",
    "            \n",
    "    from all_player_rows\n",
    "    group by player_id, is_male\n",
    "    having is_male=0\n",
    "\"\"\")\n",
    "feature_df = spark.sql(feature_query_template.substitute(venue_name=\"St George\\\\'s Park, Gqeberha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6060934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "feature_df_with_bat_avg = feature_df\\\n",
    "        .withColumn(\"batting_avg_30D\", f.when(f.col(\"dismissals_30D\") > 0, f.col(\"batter_runs_30D\")/f.col(\"dismissals_30D\")).otherwise(f.col(\"batter_runs_30D\"))) \\\n",
    "        .withColumn(\"batting_avg_90D\", f.when(f.col(\"dismissals_90D\") > 0, f.col(\"batter_runs_90D\")/f.col(\"dismissals_90D\")).otherwise(f.col(\"batter_runs_90D\"))) \\\n",
    "        .withColumn(\"batting_avg_300D\", f.when(f.col(\"dismissals_300D\") > 0, f.col(\"batter_runs_300D\")/f.col(\"dismissals_300D\")).otherwise(f.col(\"batter_runs_300D\"))) \\\n",
    "        .withColumn(\"batting_avg_1000D\", f.when(f.col(\"dismissals_1000D\") > 0, f.col(\"batter_runs_1000D\")/f.col(\"dismissals_1000D\")).otherwise(f.col(\"batter_runs_1000D\"))) \\\n",
    "        .withColumn(\"batting_avg_1000D_venue\", f.when(f.col(\"dismissals_1000D_venue\") > 0, f.col(\"batter_runs_1000D_venue\")/f.col(\"dismissals_1000D_venue\")).otherwise(f.col(\"batter_runs_1000D_venue\")))\n",
    "\n",
    "feature_df_with_bat_avg_sr = feature_df_with_bat_avg\\\n",
    "    .withColumn(\"batting_sr_30D\", f.when(f.col(\"balls_faced_30D\") > 0, f.col(\"batter_runs_30D\")/f.col(\"balls_faced_30D\")).otherwise(f.col(\"batter_runs_30D\"))) \\\n",
    "    .withColumn(\"batting_sr_90D\", f.when(f.col(\"balls_faced_90D\") > 0, f.col(\"batter_runs_90D\")/f.col(\"balls_faced_90D\")).otherwise(f.col(\"batter_runs_90D\"))) \\\n",
    "    .withColumn(\"batting_sr_300D\", f.when(f.col(\"balls_faced_300D\") > 0, f.col(\"batter_runs_300D\")/f.col(\"balls_faced_300D\")).otherwise(f.col(\"batter_runs_300D\"))) \\\n",
    "    .withColumn(\"batting_sr_1000D\", f.when(f.col(\"balls_faced_1000D\") > 0, f.col(\"batter_runs_1000D\")/f.col(\"balls_faced_1000D\")).otherwise(f.col(\"batter_runs_1000D\"))) \\\n",
    "    .withColumn(\"batting_sr_1000D_venue\", f.when(f.col(\"balls_faced_1000D_venue\") > 0, f.col(\"batter_runs_1000D_venue\")/f.col(\"balls_faced_1000D_venue\")).otherwise(f.col(\"batter_runs_1000D_venue\")))\n",
    "\n",
    "feature_df_with_bat_avg_sr_bowl_avg = feature_df_with_bat_avg_sr\\\n",
    "        .withColumn(\"bowling_avg_30D\", f.when(f.col(\"wicket_sum_30D\") > 0, f.col(\"total_runs_30D\")/f.col(\"wicket_sum_30D\")).otherwise(f.col(\"total_runs_30D\"))) \\\n",
    "        .withColumn(\"bowling_avg_90D\", f.when(f.col(\"wicket_sum_90D\") > 0, f.col(\"total_runs_90D\")/f.col(\"wicket_sum_90D\")).otherwise(f.col(\"total_runs_90D\"))) \\\n",
    "        .withColumn(\"bowling_avg_300D\", f.when(f.col(\"wicket_sum_300D\") > 0, f.col(\"total_runs_300D\")/f.col(\"wicket_sum_300D\")).otherwise(f.col(\"total_runs_300D\"))) \\\n",
    "        .withColumn(\"bowling_avg_1000D\", f.when(f.col(\"wicket_sum_1000D\") > 0, f.col(\"total_runs_1000D\")/f.col(\"wicket_sum_1000D\")).otherwise(f.col(\"total_runs_1000D\"))) \\\n",
    "        .withColumn(\"bowling_avg_1000D_venue\", f.when(f.col(\"wicket_sum_1000D_venue\") > 0, f.col(\"total_runs_1000D_venue\")/f.col(\"wicket_sum_1000D_venue\")).otherwise(f.col(\"total_runs_1000D_venue\")))\n",
    "\n",
    "feature_df_with_bat_avg_sr_bowl_avg_sr = feature_df_with_bat_avg_sr_bowl_avg\\\n",
    "    .withColumn(\"bowling_sr_30D\", f.when(f.col(\"wicket_sum_30D\") > 0, f.col(\"deliveries_30D\")/f.col(\"wicket_sum_30D\")).otherwise(f.col(\"deliveries_30D\"))) \\\n",
    "    .withColumn(\"bowling_sr_90D\", f.when(f.col(\"wicket_sum_90D\") > 0, f.col(\"deliveries_90D\")/f.col(\"wicket_sum_90D\")).otherwise(f.col(\"deliveries_90D\"))) \\\n",
    "    .withColumn(\"bowling_sr_300D\", f.when(f.col(\"wicket_sum_300D\") > 0, f.col(\"deliveries_300D\")/f.col(\"wicket_sum_300D\")).otherwise(f.col(\"deliveries_300D\"))) \\\n",
    "    .withColumn(\"bowling_sr_1000D\", f.when(f.col(\"wicket_sum_1000D\") > 0, f.col(\"deliveries_1000D\")/f.col(\"wicket_sum_1000D\")).otherwise(f.col(\"deliveries_1000D\"))) \\\n",
    "    .withColumn(\"bowling_sr_1000D_venue\", f.when(f.col(\"wicket_sum_1000D_venue\") > 0, f.col(\"deliveries_1000D_venue\")/f.col(\"wicket_sum_1000D_venue\")).otherwise(f.col(\"deliveries_1000D_venue\")))\n",
    "\n",
    "all_features_df = feature_df_with_bat_avg_sr_bowl_avg_sr\\\n",
    "    .withColumn(\"bowling_eco_30D\", f.when(f.col(\"deliveries_30D\") > 0, f.col(\"total_runs_30D\") * 6.0/f.col(\"deliveries_30D\")).otherwise(f.col(\"total_runs_30D\") * 6.0)) \\\n",
    "    .withColumn(\"bowling_eco_90D\", f.when(f.col(\"deliveries_90D\") > 0, f.col(\"total_runs_90D\") * 6.0/f.col(\"deliveries_90D\")).otherwise(f.col(\"total_runs_90D\") * 6.0)) \\\n",
    "    .withColumn(\"bowling_eco_300D\", f.when(f.col(\"deliveries_300D\") > 0, f.col(\"total_runs_300D\") * 6.0/f.col(\"deliveries_300D\")).otherwise(f.col(\"total_runs_300D\") * 6.0)) \\\n",
    "    .withColumn(\"bowling_eco_1000D\", f.when(f.col(\"deliveries_1000D\") > 0, f.col(\"total_runs_1000D\") * 6.0/f.col(\"deliveries_1000D\")).otherwise(f.col(\"total_runs_1000D\") * 6.0)) \\\n",
    "    .withColumn(\"bowling_eco_1000D_venue\", f.when(f.col(\"deliveries_1000D_venue\") > 0, f.col(\"total_runs_1000D_venue\") * 6.0/f.col(\"deliveries_1000D_venue\")).otherwise(f.col(\"total_runs_1000D_venue\") * 6.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_last_names = [\n",
    "    \"healy\",\n",
    "    \"mooney\",\n",
    "    \"jafta\",\n",
    "    \"tucker\",\n",
    "    \"lanning\",\n",
    "    \"wolvaardt\",\n",
    "    \"luus\",\n",
    "    \"mcgrath\",\n",
    "    \"bosch\",\n",
    "    \"harris\",\n",
    "    \"goodall\",\n",
    "    \"brits\",\n",
    "    \"dercksen\",\n",
    "    \"macheke\",\n",
    "    \"perry\",\n",
    "    \"kapp\",\n",
    "    \"gardner\",\n",
    "    \"klerk\",\n",
    "    \"tryon\",\n",
    "    \"sutherland\",\n",
    "    \"garth\",\n",
    "    \"andrews\",\n",
    "    \"schutt\",\n",
    "    \"jonassen\",\n",
    "    \"ismail\",\n",
    "    \"klaas\",\n",
    "    \"king\",\n",
    "    \"brown\",\n",
    "    \"khaka\",\n",
    "    \"wareham\",\n",
    "    \"mlaba\",\n",
    "    \"graham\",\n",
    "    \"sekhumkhune\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_df = spark.read.csv(\"downloads/people.csv\", header=True)\n",
    "all_players_df.registerTempTable(\"all_players\")\n",
    "all_players_df.toPandas()\n",
    "\n",
    "from string import Template\n",
    "name_predicates = []\n",
    "name_clause_template = Template(\"lower(reverse(split(unique_name,' '))[0]) = '$player_name'\")\n",
    "for name in player_last_names:\n",
    "    name_predicates.append(name_clause_template.substitute(player_name=name))\n",
    "name_clause = \" OR \".join(name_predicates)\n",
    "player_id_names_df = spark.sql(\"select identifier, unique_name from all_players where %s\"%name_clause)\n",
    "\n",
    "player_features_df = all_features_df.join(\n",
    "    player_id_names_df,\n",
    "    all_features_df.player_id ==  player_id_names_df.identifier,\"inner\" )\n",
    "player_features_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3061a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor ,GBTRegressionModel\n",
    "from path_manager import model_train_input_path, model_test_input_path, model_train_predictions_path, model_test_predictions_path, model_save_artifact_path\n",
    "from model_trainer import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_gbtr = GBTRegressionModel.load(model_save_artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols = features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ff96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_inference_df = va.transform(player_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_predictions_df = loaded_gbtr.transform(va_inference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_predictions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b8d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_predictions_df.registerTempTable(\"player_inferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select player_id, unique_name, \n",
    "            prediction, rank() over(order by prediction desc) as predicted_rank\n",
    "        from player_inferences\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c9151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "198575fd6629fa91bfd9fa074cc83b4a356efd4da6eb166ac1d1445fe6d3a8d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
